{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahinlma/pytorch_quantization/blob/main/pytorch_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "integrated-collar",
      "metadata": {
        "id": "integrated-collar",
        "outputId": "479028b6-a669-43ee-e82c-a5b5a0c4f837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assisted-mistress",
      "metadata": {
        "id": "assisted-mistress"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "Here's a sample execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "practical-theme",
      "metadata": {
        "id": "practical-theme"
      },
      "outputs": [],
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model):\n",
        "  with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "  \n",
        "  probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "  results(probabilities)"
      ],
      "metadata": {
        "id": "x6JQ5OpudFMn"
      },
      "id": "x6JQ5OpudFMn",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "behavioral-portal",
      "metadata": {
        "id": "behavioral-portal"
      },
      "outputs": [],
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "# if torch.cuda.is_available():\n",
        "#     input_batch = input_batch.to('cuda')\n",
        "#     model.to('cuda')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "perceived-federation",
      "metadata": {
        "id": "perceived-federation",
        "outputId": "354cf84b-0c60-4e5b-a635-672eaede6fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-21 08:51:47--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt.1’\n",
            "\n",
            "\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-21 08:51:48 (52.8 MB/s) - ‘imagenet_classes.txt.1’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "variable-thomas",
      "metadata": {
        "id": "variable-thomas"
      },
      "outputs": [],
      "source": [
        "# Read the categories\n",
        "def results(probabilities):\n",
        "  with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "      categories = [s.strip() for s in f.readlines()]\n",
        "  # Show top categories per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  for i in range(top5_prob.size(0)):\n",
        "      print(categories[top5_catid[i]], top5_prob[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "model_fp32 = copy.deepcopy(model)\n",
        "torchscript_model = torch.jit.script(model_fp32.eval())\n",
        "#print(torchscript_model)"
      ],
      "metadata": {
        "id": "h2vUeA-fa_T-"
      },
      "id": "h2vUeA-fa_T-",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inference(torchscript_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejvdmOXPbbbt",
        "outputId": "a5594b89-c78c-4272-d4d9-0cc79332d5b5"
      },
      "id": "ejvdmOXPbbbt",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.8846225142478943\n",
            "Arctic fox 0.045805174857378006\n",
            "white wolf 0.0442761555314064\n",
            "Pomeranian 0.005621383432298899\n",
            "Great Pyrenees 0.004652013536542654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchscript_model_opt = torch.jit.optimize_for_inference(torchscript_model)\n",
        "print(torchscript_model_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4AEEbkpcFUd",
        "outputId": "01d75b32-351f-4125-d90d-a02fd1056dd8"
      },
      "id": "V4AEEbkpcFUd",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveScriptModule(original_name=ResNet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(torchscript_model_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi-WzfPqcRzo",
        "outputId": "5cb1dcbf-ae0d-4372-9f90-39ea36fa3588"
      },
      "id": "mi-WzfPqcRzo",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.8846223950386047\n",
            "Arctic fox 0.04580499231815338\n",
            "white wolf 0.044276442378759384\n",
            "Pomeranian 0.005621393211185932\n",
            "Great Pyrenees 0.004652039613574743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = copy.deepcopy(model)\n",
        "\n",
        "resnet_dq_int8 = torch.quantization.quantize_dynamic(\n",
        "    model_fp32,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8) "
      ],
      "metadata": {
        "id": "nHE9NNLoctws"
      },
      "id": "nHE9NNLoctws",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(resnet_dq_int8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_NbXvadcx3p",
        "outputId": "c5d54c88-5b66-4405-edd7-e1b1874e8ece"
      },
      "id": "i_NbXvadcx3p",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.883510947227478\n",
            "Arctic fox 0.047078829258680344\n",
            "white wolf 0.04523681104183197\n",
            "Pomeranian 0.005195914302021265\n",
            "Great Pyrenees 0.004503224976360798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchscript_model_int8 = torch.jit.script(resnet_dq_int8)\n"
      ],
      "metadata": {
        "id": "SNmHveP8e2Tq"
      },
      "id": "SNmHveP8e2Tq",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(torchscript_model_int8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWeuG30Se-h-",
        "outputId": "7c8f6852-ffa9-40cf-fcf0-2a30c22b11d3"
      },
      "id": "dWeuG30Se-h-",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.883510947227478\n",
            "Arctic fox 0.047078829258680344\n",
            "white wolf 0.04523681104183197\n",
            "Pomeranian 0.005195914302021265\n",
            "Great Pyrenees 0.004503224976360798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchscript_model_int8_1 = torch.jit.optimize_for_inference(torchscript_model_int8)\n",
        "print(torchscript_model_int8_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN9Z0I76fYCP",
        "outputId": "95b0ed97-843f-4710-c63c-8fd2ce3c85f8"
      },
      "id": "jN9Z0I76fYCP",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveScriptModule(original_name=ResNet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(torchscript_model_int8_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-jRx8e9ffAi",
        "outputId": "fef68946-945c-40e1-d302-ac01c10ad9dd"
      },
      "id": "H-jRx8e9ffAi",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.883510947227478\n",
            "Arctic fox 0.04707878455519676\n",
            "white wolf 0.04523677006363869\n",
            "Pomeranian 0.005195904523134232\n",
            "Great Pyrenees 0.004503220319747925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchscript_quant_model = torch.quantization.prepare_jit(torchscript_model, {\"\":torch.quantization.default_qconfig}, True)\n",
        "\n",
        "for _ in range(100):\n",
        "    torchscript_quant_model(input_batch)\n",
        "\n",
        "torchscript_quant_model = torch.quantization.convert_jit(torchscript_quant_model)\n",
        "print(torchscript_quant_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLIzbMbxf3nd",
        "outputId": "0d2be02e-c38f-436b-8184-828d98371b23"
      },
      "id": "TLIzbMbxf3nd",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveScriptModule(original_name=ResNet)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchscript_quant_model.graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWqqyFyjgizt",
        "outputId": "dd6f8fb3-9db4-459f-f251-1498c70bf135"
      },
      "id": "LWqqyFyjgizt",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "graph(%self : __torch__.torchvision.models.resnet.___torch_mangle_780.ResNet,\n",
              "      %x.2 : Tensor):\n",
              "  %15 : int = prim::Constant[value=-1]()\n",
              "  %14 : int = prim::Constant[value=1]() # /root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:243:29\n",
              "  %13 : int[] = prim::Constant[value=[2, 2]]()\n",
              "  %12 : int[] = prim::Constant[value=[3, 3]]()\n",
              "  %11 : int[] = prim::Constant[value=[1, 1]]()\n",
              "  %10 : int = prim::Constant[value=2]() # /usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py:162:53\n",
              "  %9 : bool = prim::Constant[value=0]() # /usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py:163:57\n",
              "  %7 : str = prim::Constant[value=\"AssertionError: \"]()\n",
              "  %6 : NoneType = prim::Constant()\n",
              "  %self.4_zero_point_0 : int = prim::Constant[value=38]()\n",
              "  %self.4_scale_0 : float = prim::Constant[value=0.18226641416549683]()\n",
              "  %self.layer4.1.out.26_scale_0 : float = prim::Constant[value=0.16433815658092499]()\n",
              "  %self.layer4.1.out.30_zero_point_0 : int = prim::Constant[value=37]()\n",
              "  %self.layer4.1.out.30_scale_0 : float = prim::Constant[value=0.21064890921115875]()\n",
              "  %self.layer4.1.out.28_scale_0 : float = prim::Constant[value=0.011596559546887875]()\n",
              "  %self.layer4.0.out.26_scale_0 : float = prim::Constant[value=0.027313809841871262]()\n",
              "  %self.layer4.0.identity.2_zero_point_0 : int = prim::Constant[value=61]()\n",
              "  %self.layer4.0.identity.2_scale_0 : float = prim::Constant[value=0.026991883292794228]()\n",
              "  %self.layer4.0.out.30_scale_0 : float = prim::Constant[value=0.048367910087108612]()\n",
              "  %self.layer4.0.out.28_scale_0 : float = prim::Constant[value=0.020205806940793991]()\n",
              "  %self.layer3.1.out.26_scale_0 : float = prim::Constant[value=0.030107058584690094]()\n",
              "  %self.layer3.1.out.30_zero_point_0 : int = prim::Constant[value=77]()\n",
              "  %self.layer3.1.out.30_scale_0 : float = prim::Constant[value=0.051543362438678741]()\n",
              "  %self.layer3.1.out.28_scale_0 : float = prim::Constant[value=0.018147584050893784]()\n",
              "  %self.layer3.0.out.26_scale_0 : float = prim::Constant[value=0.022435560822486877]()\n",
              "  %self.layer3.0.identity.2_scale_0 : float = prim::Constant[value=0.012485047802329063]()\n",
              "  %self.layer3.0.out.30_zero_point_0 : int = prim::Constant[value=58]()\n",
              "  %self.layer3.0.out.30_scale_0 : float = prim::Constant[value=0.042219310998916626]()\n",
              "  %self.layer3.0.out.28_scale_0 : float = prim::Constant[value=0.021412799134850502]()\n",
              "  %self.layer2.1.out.26_scale_0 : float = prim::Constant[value=0.039323359727859497]()\n",
              "  %self.layer2.1.out.30_zero_point_0 : int = prim::Constant[value=79]()\n",
              "  %self.layer2.1.out.30_scale_0 : float = prim::Constant[value=0.04548109695315361]()\n",
              "  %self.layer2.1.out.28_scale_0 : float = prim::Constant[value=0.012380165047943592]()\n",
              "  %self.layer2.0.out.26_scale_0 : float = prim::Constant[value=0.03599751740694046]()\n",
              "  %self.layer2.0.identity.2_zero_point_0 : int = prim::Constant[value=48]()\n",
              "  %self.layer2.0.identity.2_scale_0 : float = prim::Constant[value=0.04316508024930954]()\n",
              "  %self.layer2.0.out.30_zero_point_0 : int = prim::Constant[value=43]()\n",
              "  %self.layer2.0.out.30_scale_0 : float = prim::Constant[value=0.049492914229631424]()\n",
              "  %self.layer2.0.out.28_scale_0 : float = prim::Constant[value=0.015290195122361183]()\n",
              "  %self.layer1.1.out.26_scale_0 : float = prim::Constant[value=0.037418082356452942]()\n",
              "  %self.layer1.1.out.30_zero_point_0 : int = prim::Constant[value=80]()\n",
              "  %self.layer1.1.out.30_scale_0 : float = prim::Constant[value=0.055423296988010406]()\n",
              "  %self.layer1.1.out.28_scale_0 : float = prim::Constant[value=0.017884954810142517]()\n",
              "  %self.layer1.0.out.26_scale_0 : float = prim::Constant[value=0.026280708611011505]()\n",
              "  %self.layer1.0.out.30_zero_point_0 : int = prim::Constant[value=75]()\n",
              "  %self.layer1.0.out.30_scale_0 : float = prim::Constant[value=0.04812365397810936]()\n",
              "  %self.layer1.0.out.28_scale_0 : float = prim::Constant[value=0.010387951508164406]()\n",
              "  %self.x.14_scale_0 : float = prim::Constant[value=0.018611624836921692]()\n",
              "  %self.conv1.weight.1_zero_point_0 : int = prim::Constant[value=0]()\n",
              "  %self.x.2_scalar_type_0 : int = prim::Constant[value=13]()\n",
              "  %self.x.2_zero_point_0 : int = prim::Constant[value=56]()\n",
              "  %self.x.2_scale_0 : float = prim::Constant[value=0.037328973412513733]()\n",
              "  %x.2.quant : Tensor = aten::quantize_per_tensor(%x.2, %self.x.2_scale_0, %self.x.2_zero_point_0, %self.x.2_scalar_type_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_0 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_0\"](%self)\n",
              "  %27 : Tensor = quantized::conv2d_relu(%x.2.quant, %quantized_forward._jit_pass_packed_weight_0, %self.x.14_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %x.17 : Tensor = aten::max_pool2d(%27, %12, %13, %11, %11, %9) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:797:11\n",
              "  %quantized_forward._jit_pass_packed_weight_1 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_1\"](%self)\n",
              "  %47 : Tensor = quantized::conv2d_relu(%x.17, %quantized_forward._jit_pass_packed_weight_1, %self.layer1.0.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_2 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_2\"](%self)\n",
              "  %59 : Tensor = quantized::conv2d(%47, %quantized_forward._jit_pass_packed_weight_2, %self.layer1.0.out.30_scale_0, %self.layer1.0.out.30_zero_point_0)\n",
              "  %63 : Tensor = quantized::add_relu(%59, %x.17, %self.layer1.0.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_3 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_3\"](%self)\n",
              "  %75 : Tensor = quantized::conv2d_relu(%63, %quantized_forward._jit_pass_packed_weight_3, %self.layer1.1.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_4 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_4\"](%self)\n",
              "  %87 : Tensor = quantized::conv2d(%75, %quantized_forward._jit_pass_packed_weight_4, %self.layer1.1.out.30_scale_0, %self.layer1.1.out.30_zero_point_0)\n",
              "  %91 : Tensor = quantized::add_relu(%87, %63, %self.layer1.1.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_5 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_5\"](%self)\n",
              "  %106 : Tensor = quantized::conv2d_relu(%91, %quantized_forward._jit_pass_packed_weight_5, %self.layer2.0.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_6 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_6\"](%self)\n",
              "  %118 : Tensor = quantized::conv2d(%106, %quantized_forward._jit_pass_packed_weight_6, %self.layer2.0.out.30_scale_0, %self.layer2.0.out.30_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_7 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_7\"](%self)\n",
              "  %131 : Tensor = quantized::conv2d(%91, %quantized_forward._jit_pass_packed_weight_7, %self.layer2.0.identity.2_scale_0, %self.layer2.0.identity.2_zero_point_0)\n",
              "  %135 : Tensor = quantized::add_relu(%118, %131, %self.layer2.0.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_8 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_8\"](%self)\n",
              "  %147 : Tensor = quantized::conv2d_relu(%135, %quantized_forward._jit_pass_packed_weight_8, %self.layer2.1.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_9 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_9\"](%self)\n",
              "  %159 : Tensor = quantized::conv2d(%147, %quantized_forward._jit_pass_packed_weight_9, %self.layer2.1.out.30_scale_0, %self.layer2.1.out.30_zero_point_0)\n",
              "  %163 : Tensor = quantized::add_relu(%159, %135, %self.layer2.1.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_10 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_10\"](%self)\n",
              "  %178 : Tensor = quantized::conv2d_relu(%163, %quantized_forward._jit_pass_packed_weight_10, %self.layer3.0.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_11 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_11\"](%self)\n",
              "  %190 : Tensor = quantized::conv2d(%178, %quantized_forward._jit_pass_packed_weight_11, %self.layer3.0.out.30_scale_0, %self.layer3.0.out.30_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_12 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_12\"](%self)\n",
              "  %203 : Tensor = quantized::conv2d(%163, %quantized_forward._jit_pass_packed_weight_12, %self.layer3.0.identity.2_scale_0, %self.layer1.1.out.30_zero_point_0)\n",
              "  %207 : Tensor = quantized::add_relu(%190, %203, %self.layer3.0.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_13 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_13\"](%self)\n",
              "  %219 : Tensor = quantized::conv2d_relu(%207, %quantized_forward._jit_pass_packed_weight_13, %self.layer3.1.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_14 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_14\"](%self)\n",
              "  %231 : Tensor = quantized::conv2d(%219, %quantized_forward._jit_pass_packed_weight_14, %self.layer3.1.out.30_scale_0, %self.layer3.1.out.30_zero_point_0)\n",
              "  %235 : Tensor = quantized::add_relu(%231, %207, %self.layer3.1.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_15 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_15\"](%self)\n",
              "  %250 : Tensor = quantized::conv2d_relu(%235, %quantized_forward._jit_pass_packed_weight_15, %self.layer4.0.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_16 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_16\"](%self)\n",
              "  %262 : Tensor = quantized::conv2d(%250, %quantized_forward._jit_pass_packed_weight_16, %self.layer4.0.out.30_scale_0, %self.layer2.1.out.30_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_17 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_17\"](%self)\n",
              "  %275 : Tensor = quantized::conv2d(%235, %quantized_forward._jit_pass_packed_weight_17, %self.layer4.0.identity.2_scale_0, %self.layer4.0.identity.2_zero_point_0)\n",
              "  %279 : Tensor = quantized::add_relu(%262, %275, %self.layer4.0.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_18 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_18\"](%self)\n",
              "  %291 : Tensor = quantized::conv2d_relu(%279, %quantized_forward._jit_pass_packed_weight_18, %self.layer4.1.out.28_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %quantized_forward._jit_pass_packed_weight_19 : __torch__.torch.classes.quantized.Conv2dPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_19\"](%self)\n",
              "  %303 : Tensor = quantized::conv2d(%291, %quantized_forward._jit_pass_packed_weight_19, %self.layer4.1.out.30_scale_0, %self.layer4.1.out.30_zero_point_0)\n",
              "  %307 : Tensor = quantized::add_relu(%303, %279, %self.layer4.1.out.26_scale_0, %self.conv1.weight.1_zero_point_0)\n",
              "  %309 : Tensor = aten::adaptive_avg_pool2d(%307, %11)\n",
              "  %310 : int[] = aten::size(%307) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1240:51\n",
              "  %311 : int = aten::len(%310) # <string>:5:9\n",
              "  %312 : bool = aten::gt(%311, %10) # <string>:5:9\n",
              "   = prim::If(%312) # <string>:5:2\n",
              "    block0():\n",
              "      -> ()\n",
              "    block1():\n",
              "       = prim::RaiseException(%7, %6) # <string>:5:2\n",
              "      -> ()\n",
              "  %x.41 : Tensor = aten::flatten(%309, %14, %15) # /root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:243:12\n",
              "  %quantized_forward._jit_pass_packed_weight_20 : __torch__.torch.classes.quantized.LinearPackedParamsBase = prim::GetAttr[name=\"quantized_forward._jit_pass_packed_weight_20\"](%self)\n",
              "  %325 : Tensor = quantized::linear(%x.41, %quantized_forward._jit_pass_packed_weight_20, %self.4_scale_0, %self.4_zero_point_0)\n",
              "  %4.dequant.0 : Tensor = aten::dequantize(%325)\n",
              "  return (%4.dequant.0)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(torchscript_quant_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIl-xxGAf-CX",
        "outputId": "35a9afe3-4311-4e91-e1ea-a494464f984d"
      },
      "id": "EIl-xxGAf-CX",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.9468300938606262\n",
            "Arctic fox 0.0206048171967268\n",
            "white wolf 0.017171630635857582\n",
            "Pomeranian 0.00399533286690712\n",
            "Great Pyrenees 0.00399533286690712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z0Wo4sLg0aN",
        "outputId": "b63b5bbd-f415-4849-ea5f-67db4adef2d0"
      },
      "id": "8Z0Wo4sLg0aN",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.9468300938606262\n",
            "Arctic fox 0.0206048171967268\n",
            "white wolf 0.017171630635857582\n",
            "Pomeranian 0.00399533286690712\n",
            "Great Pyrenees 0.00399533286690712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "xPHOSvz6gIgr",
        "outputId": "00b3a83d-02b1-450a-d374-717449583412"
      },
      "id": "xPHOSvz6gIgr",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-29c8f901ca6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchscript_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_jit() missing 1 required positional argument: 'qconfig_dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def fuse_layers(model_fp32):\n",
        "import torchvision\n",
        "#model_fp32 = copy.deepcopy(model)\n",
        "#torch.quantization.fuse_modules(model_fp32, [[conv1],[bn1]])\n",
        "#model_fp32.eval()\n",
        "def fuse_model(model_fp32):\n",
        "    torch.quantization.fuse_modules(model_fp32, ['conv1','bn1','relu'],inplace=True)\n",
        "\n",
        "    for m in model_fp32.modules():\n",
        "        \n",
        "        if type(m) == torch.nn.modules.container.Sequential:\n",
        "\n",
        "            for m1 in m:\n",
        "                #print(type(m1))\n",
        "\n",
        "                if (type(m1)) == torchvision.models.resnet.BasicBlock:\n",
        "\n",
        "                    torch.quantization.fuse_modules(m1, [['conv1','bn1','relu'],['conv2','bn2']],inplace=True)\n",
        "                    if m1.downsample is not None and type(m1.downsample) == torch.nn.modules.container.Sequential:\n",
        "                        torch.quantization.fuse_modules(m1.downsample, ['0','1'],inplace=True)\n",
        "\n",
        "                        #print(\"#\",m1.downsample)\n",
        "                    #if (type(m1)) ==\n",
        "\n",
        "    return model_fp32\n",
        "\n"
      ],
      "metadata": {
        "id": "ojuPo5iTdwYD"
      },
      "id": "ojuPo5iTdwYD",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedResNet18(torch.nn.Module):\n",
        "    def __init__(self,model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Y75IFlsBeAoH"
      },
      "id": "Y75IFlsBeAoH",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = copy.deepcopy(model)\n",
        "model_fp32.eval()\n",
        "fused_model = fuse_model(model_fp32)\n",
        "\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "                \n",
        "quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "\n",
        "model_fp32_prepared = torch.quantization.prepare(quantized_model, inplace=True)\n",
        "                           \n",
        "model_fp32_prepared(input_batch)\n",
        "\n",
        "model_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True)\n",
        "\n",
        "print(model_int8)\n",
        "#torch.save(model_int8, \"resnet_static_quantized_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9qK6SIud8V3",
        "outputId": "43112caa-f786-4c64-f0c6-970729120557"
      },
      "id": "w9qK6SIud8V3",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0373]), zero_point=tensor([56]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.017348434776067734, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009992317296564579, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.046314314007759094, zero_point=73, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.021600106731057167, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.09933473914861679, zero_point=72, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.04198942333459854, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1540270298719406, zero_point=76, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.06361019611358643, zero_point=61)\n",
            "          (1): Identity()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.106589674949646, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.5590881705284119, zero_point=67, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.36174124479293823, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.8945695161819458, zero_point=75, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.17314112186431885, zero_point=60)\n",
            "          (1): Identity()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.814084768295288, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=24.157703399658203, zero_point=56, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=21.03162384033203, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=328.2539978027344, zero_point=67, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=14.650483131408691, zero_point=68)\n",
            "          (1): Identity()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=863.8945922851562, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=44671.5234375, zero_point=66, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=1000, scale=60868.09375, zero_point=68, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model_int8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hoZh3bgred22",
        "outputId": "b2a644be-4b8e-4fde-acc1-8176db047431"
      },
      "id": "hoZh3bgred22",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-25d885553b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_int8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-d45fa7923d21>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(output[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-95ce36c870ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# point to quantized in the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# manually specify where tensors will be converted from quantized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# to floating point in the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:14951 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:659 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1791 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1941 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1242 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1396 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2566 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:10104 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:12568 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:4018 [kernel]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5dFE7ncUcCKa"
      },
      "id": "5dFE7ncUcCKa",
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_quantization",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}